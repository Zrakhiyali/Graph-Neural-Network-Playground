{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchdata dgl -y\n",
        "!pip install torch==1.12.0\n",
        "!pip install dgl==0.9.0\n",
        "!pip install torchdata==0.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB3JPhioU7nM",
        "outputId": "39b44ca2-6ed4-4179-878e-57986a60913c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 1.12.0\n",
            "Uninstalling torch-1.12.0:\n",
            "  Successfully uninstalled torch-1.12.0\n",
            "Found existing installation: torchdata 0.4.0\n",
            "Uninstalling torchdata-0.4.0:\n",
            "  Successfully uninstalled torchdata-0.4.0\n",
            "Found existing installation: dgl 0.9.0\n",
            "Uninstalling dgl-0.9.0:\n",
            "  Successfully uninstalled dgl-0.9.0\n",
            "Collecting torch==1.12.0\n",
            "  Using cached torch-1.12.0-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.0) (4.12.1)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.0\n",
            "Collecting dgl==0.9.0\n",
            "  Using cached dgl-0.9.0-cp310-cp310-manylinux1_x86_64.whl (6.2 MB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (4.66.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (2024.6.2)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.9.0\n",
            "Collecting torchdata==0.4.0\n",
            "  Using cached torchdata-0.4.0-cp310-cp310-manylinux2014_x86_64.whl (4.4 MB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.4.0) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.4.0) (2.8.2)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.4.0) (1.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.0->torchdata==0.4.0) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.4.0) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.4.0) (2024.6.2)\n",
            "Installing collected packages: torchdata\n",
            "Successfully installed torchdata-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['DGLBACKEND'] = 'pytorch'\n",
        "\n",
        "import torch\n",
        "import dgl"
      ],
      "metadata": {
        "id": "VkCKITkHUHvB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0MdcTNmsqt93"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " torch.manual_seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQioff_VUe8S",
        "outputId": "2ed897f6-a031-4936-e73c-1f881b2dd588"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ed4b917a790>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_nodes = 20\n",
        "in_feats = 8\n",
        "hid_feats = 32\n",
        "out_feats = 2"
      ],
      "metadata": {
        "id": "eDUM-jq1gU4s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = torch.randint(low=0, high=num_nodes, size=(60,))\n",
        "d = torch.randint(low=0, high=num_nodes, size=(60,))"
      ],
      "metadata": {
        "id": "7WorbzpOXQcs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myGraph = dgl.graph((s,d))"
      ],
      "metadata": {
        "id": "evq5DrY7VVry"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_features = torch.randn(num_nodes, in_feats)\n",
        "myGraph.ndata['features'] = node_features\n",
        "edge_weights = torch.randn(60)\n",
        "myGraph.edata['weights'] = edge_weights\n",
        "myGraph.edge_index=torch.stack([s, d], dim=0)"
      ],
      "metadata": {
        "id": "9oIEcet0aexV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.randint(0, 2, (num_nodes,))\n",
        "myGraph.ndata['labels'] = labels"
      ],
      "metadata": {
        "id": "2DRc5dBTbaJf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mask = torch.ones(num_nodes, dtype=torch.bool)\n",
        "myGraph.ndata['train_mask'] = train_mask"
      ],
      "metadata": {
        "id": "m872b_9bdo0v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myGraph.ndata['train_mask'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We9-gvFTOIYG",
        "outputId": "0521cdc3-34a2-4f58-f5f3-f34e82ccd036"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dojgi0h3N3J8",
        "outputId": "fcd699dd-2d4e-47c6-e8ad-f80dac4054b5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)"
      ],
      "metadata": {
        "id": "Wbp709M4N9_i"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfgaWGBjN_Tv",
        "outputId": "26b2a832-8e0f-4c23-b41a-7145cb5fed8b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_mask[:10] = True\n",
        "val_mask[10:15] = True\n",
        "test_mask[15:] = True\n",
        "myGraph.ndata['val_mask'] = val_mask\n",
        "myGraph.ndata['test_mask'] = test_mask"
      ],
      "metadata": {
        "id": "Bi5GrPgpjuwq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myGraph.ndata['val_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RtreVbYNx9I",
        "outputId": "1adb376c-254d-4bf3-a3e5-24aef59c031f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False, False, False, False, False, False, False, False, False, False,\n",
              "         True,  True,  True,  True,  True, False, False, False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myGraph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmpgsYckZm5B",
        "outputId": "58424de9-4544-48b3-d752-4f7a77a58db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph(num_nodes=20, num_edges=60,\n",
              "      ndata_schemes={}\n",
              "      edata_schemes={})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn import GraphConv"
      ],
      "metadata": {
        "id": "7Bzjdiz0bbjJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "  def __init__(self,in_feat,hid_feat,out_feat):\n",
        "     super(GCN, self).__init__()\n",
        "     self.conv1=GraphConv(in_feat,hid_feat)\n",
        "     self.conv2=GraphConv(hid_feat,out_feat)\n",
        "     self.dropout=nn.Dropout(0.2)\n",
        "\n",
        "  def forward (self,g,d_rate=0.2):\n",
        "        x = self.conv1(g,g.ndata['features'])\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=d_rate, training=self.training)\n",
        "        x = self.conv2(g, x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "aU1q4umPb4Bp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(in_feats,hid_feats,out_feats)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=.02)\n",
        "schedulerGCN = StepLR(optimizer, step_size=10, gamma=0.5)"
      ],
      "metadata": {
        "id": "ZBD4bi-QdY8x"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=100"
      ],
      "metadata": {
        "id": "pIW4zNJceZc1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_lossGCN = float('inf')\n",
        "patienceGCN = 10\n",
        "min_deltaGCN = 0.01\n",
        "counterGCN = 0"
      ],
      "metadata": {
        "id": "FHYycoYMrHSc"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(num_epochs):\n",
        "  model.train()\n",
        "  logits = model(myGraph)\n",
        "  loss = loss_fn(logits[myGraph.ndata['train_mask']], myGraph.ndata['labels'][myGraph.ndata['train_mask']])\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    val_logits = model(myGraph)\n",
        "    val_loss = loss_fn(val_logits[myGraph.ndata['val_mask']], myGraph.ndata['labels'][myGraph.ndata['val_mask']])\n",
        "\n",
        "  schedulerGCN.step()\n",
        "  if val_loss < best_val_lossGCN - min_deltaGCN:\n",
        "        best_val_lossGCN = val_loss\n",
        "        counterGCN = 0\n",
        "  else:\n",
        "        counterGCN += 1\n",
        "\n",
        "  if counterGCN >= patienceGCN:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "  print(\"for epoch\",e,\"  train loss is \",loss.item(),\"     val loss  \",val_loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxgNyMlfefTj",
        "outputId": "ea54b247-e77a-4f0f-ec02-290a75a59299"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for epoch 0   train loss is  0.3657657504081726      val loss   0.6731125712394714\n",
            "for epoch 1   train loss is  0.36390408873558044      val loss   0.648643434047699\n",
            "for epoch 2   train loss is  0.3582181930541992      val loss   0.6239619255065918\n",
            "for epoch 3   train loss is  0.34318870306015015      val loss   0.5991392135620117\n",
            "for epoch 4   train loss is  0.3498222231864929      val loss   0.5686957240104675\n",
            "for epoch 5   train loss is  0.30927032232284546      val loss   0.5409228205680847\n",
            "for epoch 6   train loss is  0.3153562843799591      val loss   0.5188938975334167\n",
            "for epoch 7   train loss is  0.31714102625846863      val loss   0.5098923444747925\n",
            "for epoch 8   train loss is  0.31768277287483215      val loss   0.5009978413581848\n",
            "for epoch 9   train loss is  0.3000843822956085      val loss   0.4908103048801422\n",
            "for epoch 10   train loss is  0.2776080071926117      val loss   0.4835396707057953\n",
            "for epoch 11   train loss is  0.29135847091674805      val loss   0.47190046310424805\n",
            "for epoch 12   train loss is  0.27775007486343384      val loss   0.45846280455589294\n",
            "for epoch 13   train loss is  0.29560813307762146      val loss   0.4430258274078369\n",
            "for epoch 14   train loss is  0.2759796679019928      val loss   0.42628413438796997\n",
            "for epoch 15   train loss is  0.2840849757194519      val loss   0.4090825021266937\n",
            "for epoch 16   train loss is  0.2418573796749115      val loss   0.39128273725509644\n",
            "for epoch 17   train loss is  0.25396445393562317      val loss   0.38594403862953186\n",
            "for epoch 18   train loss is  0.25944873690605164      val loss   0.38091933727264404\n",
            "for epoch 19   train loss is  0.25641101598739624      val loss   0.37535080313682556\n",
            "for epoch 20   train loss is  0.22784742712974548      val loss   0.3712417781352997\n",
            "for epoch 21   train loss is  0.2414521872997284      val loss   0.3695174753665924\n",
            "for epoch 22   train loss is  0.3122444748878479      val loss   0.36511436104774475\n",
            "for epoch 23   train loss is  0.2609691023826599      val loss   0.3609464764595032\n",
            "for epoch 24   train loss is  0.2553393542766571      val loss   0.35788312554359436\n",
            "for epoch 25   train loss is  0.24760571122169495      val loss   0.35633033514022827\n",
            "for epoch 26   train loss is  0.26183122396469116      val loss   0.35380688309669495\n",
            "for epoch 27   train loss is  0.26417431235313416      val loss   0.3522825241088867\n",
            "for epoch 28   train loss is  0.2360687255859375      val loss   0.35023370385169983\n",
            "for epoch 29   train loss is  0.22085614502429962      val loss   0.347548246383667\n",
            "for epoch 30   train loss is  0.24875065684318542      val loss   0.3439784646034241\n",
            "for epoch 31   train loss is  0.22707900404930115      val loss   0.3419424593448639\n",
            "for epoch 32   train loss is  0.22547924518585205      val loss   0.3387235701084137\n",
            "for epoch 33   train loss is  0.21822801232337952      val loss   0.336571604013443\n",
            "for epoch 34   train loss is  0.22694742679595947      val loss   0.3339235782623291\n",
            "for epoch 35   train loss is  0.251472532749176      val loss   0.33089178800582886\n",
            "for epoch 36   train loss is  0.2077401578426361      val loss   0.3292000889778137\n",
            "for epoch 37   train loss is  0.24255891144275665      val loss   0.32791194319725037\n",
            "for epoch 38   train loss is  0.22139838337898254      val loss   0.3266107738018036\n",
            "for epoch 39   train loss is  0.26190659403800964      val loss   0.32523101568222046\n",
            "for epoch 40   train loss is  0.2429230511188507      val loss   0.32393017411231995\n",
            "for epoch 41   train loss is  0.25827473402023315      val loss   0.3232327401638031\n",
            "for epoch 42   train loss is  0.22784683108329773      val loss   0.3235044479370117\n",
            "for epoch 43   train loss is  0.21984457969665527      val loss   0.3241468369960785\n",
            "for epoch 44   train loss is  0.2383698672056198      val loss   0.32458359003067017\n",
            "for epoch 45   train loss is  0.23971235752105713      val loss   0.3245396614074707\n",
            "for epoch 46   train loss is  0.20852522552013397      val loss   0.32479825615882874\n",
            "for epoch 47   train loss is  0.20835566520690918      val loss   0.32459840178489685\n",
            "for epoch 48   train loss is  0.20957854390144348      val loss   0.3242737650871277\n",
            "Early stopping triggered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  test_logits = model(myGraph)\n",
        "  test_loss = loss_fn(test_logits[myGraph.ndata['test_mask']], myGraph.ndata['labels'][myGraph.ndata['test_mask']])\n",
        "  print(f\"Test Loss = {test_loss.item()}\")\n",
        "\n",
        "  _, predicted = torch.max(test_logits[myGraph.ndata['test_mask']], dim=1)\n",
        "  accuracy = (predicted == myGraph.ndata['labels'][myGraph.ndata['test_mask']]).float().mean()\n",
        "  print(f\"Test Accuracy = {accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "qEpAh-vUkQCt",
        "outputId": "6f35b52e-e1fc-4ec6-f292-9bcf7b170333",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss = 0.21487149596214294\n",
            "Test Accuracy = 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now Try GIN**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pa9UBYcLTqiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import dgl\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn import GINConv\n",
        "from dgl.nn.pytorch import SumPooling\n"
      ],
      "metadata": {
        "id": "9J7wlxTYT3oe"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GIN(nn.Module):\n",
        "    def __init__(self, in_feat, hid_feat, out_feat):\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(\n",
        "            nn.Sequential(\n",
        "                nn.Linear(in_feat, hid_feat),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hid_feat, hid_feat),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(hid_feat),\n",
        "            ),\n",
        "            'sum'\n",
        "        )\n",
        "        self.conv2 = GINConv(\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hid_feat, out_feat),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(out_feat, out_feat),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(out_feat),\n",
        "            ),\n",
        "            'sum'\n",
        "        )\n",
        "        self.dropout=nn.Dropout(0.2)\n",
        "    def forward (self,g,d_rate=0.2):\n",
        "        x = self.conv1(g, g.ndata['features'])\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=d_rate, training=self.training)\n",
        "        x = self.conv2(g, x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "wfVcKYEpTp-w"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelGIN = GIN(in_feats,hid_feats,out_feats)\n",
        "loss_fn_GIN = nn.CrossEntropyLoss()\n",
        "optimizer_GIN = torch.optim.Adam(modelGIN.parameters(),lr=.02)\n",
        "schedulerGIN = StepLR(optimizer, step_size=10, gamma=0.5)"
      ],
      "metadata": {
        "id": "0i1KzQDEhooZ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=100"
      ],
      "metadata": {
        "id": "9m6zA2ZLiK6Q"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_lossGIN = float('inf')\n",
        "patienceGIN = 10\n",
        "min_deltaGIN = 0.01\n",
        "counterGIN = 0"
      ],
      "metadata": {
        "id": "FRGTGOsvsRJB"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(num_epochs):\n",
        "  modelGIN.train()\n",
        "  logits = modelGIN(myGraph)\n",
        "  loss = loss_fn_GIN(logits[myGraph.ndata['train_mask']], myGraph.ndata['labels'][myGraph.ndata['train_mask']])\n",
        "  optimizer_GIN.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer_GIN.step()\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    val_logits = modelGIN(myGraph)\n",
        "    val_loss = loss_fn_GIN(val_logits[myGraph.ndata['val_mask']], myGraph.ndata['labels'][myGraph.ndata['val_mask']])\n",
        "  schedulerGIN.step()\n",
        "  if val_loss < best_val_lossGIN - min_deltaGIN:\n",
        "        best_val_lossGIN = val_loss\n",
        "        counterGIN = 0\n",
        "  else:\n",
        "        counter += 1\n",
        "\n",
        "  if counterGIN >= patienceGIN:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "  print(\"for epoch\",e,\"  train loss is \",loss.item(),\"     val loss  \",val_loss.item())"
      ],
      "metadata": {
        "id": "2Q5gz_raiN0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07c238c-a57c-4830-941f-57d7b0b59c92"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for epoch 0   train loss is  0.8811634182929993      val loss   0.39225947856903076\n",
            "for epoch 1   train loss is  0.4974571168422699      val loss   0.2749970555305481\n",
            "for epoch 2   train loss is  0.46577635407447815      val loss   0.3021317720413208\n",
            "for epoch 3   train loss is  0.45242610573768616      val loss   0.30234381556510925\n",
            "for epoch 4   train loss is  0.5015394687652588      val loss   0.4417112469673157\n",
            "for epoch 5   train loss is  0.5150814056396484      val loss   0.33920222520828247\n",
            "for epoch 6   train loss is  0.4061223864555359      val loss   0.4449973702430725\n",
            "for epoch 7   train loss is  0.4126763343811035      val loss   0.3650593161582947\n",
            "for epoch 8   train loss is  0.40389910340309143      val loss   0.45945191383361816\n",
            "for epoch 9   train loss is  0.40012431144714355      val loss   0.3735613226890564\n",
            "for epoch 10   train loss is  0.3903151750564575      val loss   0.4128277897834778\n",
            "for epoch 11   train loss is  0.44320836663246155      val loss   0.3570365309715271\n",
            "for epoch 12   train loss is  0.39631325006484985      val loss   0.3165304660797119\n",
            "for epoch 13   train loss is  0.36323270201683044      val loss   0.24502119421958923\n",
            "for epoch 14   train loss is  0.3605247139930725      val loss   0.23935875296592712\n",
            "for epoch 15   train loss is  0.3565860092639923      val loss   0.2627039849758148\n",
            "for epoch 16   train loss is  0.3714636266231537      val loss   0.3138943314552307\n",
            "for epoch 17   train loss is  0.40039968490600586      val loss   0.2795315086841583\n",
            "for epoch 18   train loss is  0.35503166913986206      val loss   0.2831907570362091\n",
            "for epoch 19   train loss is  0.34219375252723694      val loss   0.24391786754131317\n",
            "for epoch 20   train loss is  0.37494778633117676      val loss   0.2861979603767395\n",
            "for epoch 21   train loss is  0.305619478225708      val loss   0.3130006194114685\n",
            "for epoch 22   train loss is  0.3137817978858948      val loss   0.2848762273788452\n",
            "for epoch 23   train loss is  0.31883102655410767      val loss   0.26756182312965393\n",
            "for epoch 24   train loss is  0.29495397210121155      val loss   0.33128413558006287\n",
            "for epoch 25   train loss is  0.3253280818462372      val loss   0.27735161781311035\n",
            "for epoch 26   train loss is  0.3083236813545227      val loss   0.3813135027885437\n",
            "for epoch 27   train loss is  0.2847211956977844      val loss   0.23056621849536896\n",
            "for epoch 28   train loss is  0.28143617510795593      val loss   0.3673608601093292\n",
            "for epoch 29   train loss is  0.28640857338905334      val loss   0.2760787606239319\n",
            "for epoch 30   train loss is  0.2536297142505646      val loss   0.23068828880786896\n",
            "for epoch 31   train loss is  0.2549126148223877      val loss   0.18667154014110565\n",
            "for epoch 32   train loss is  0.2617623209953308      val loss   0.30373287200927734\n",
            "for epoch 33   train loss is  0.23788675665855408      val loss   0.22273893654346466\n",
            "for epoch 34   train loss is  0.252593457698822      val loss   0.32709407806396484\n",
            "for epoch 35   train loss is  0.22677560150623322      val loss   0.22044532001018524\n",
            "for epoch 36   train loss is  0.27774450182914734      val loss   0.16172486543655396\n",
            "for epoch 37   train loss is  0.23161713778972626      val loss   0.244937464594841\n",
            "for epoch 38   train loss is  0.2554762661457062      val loss   0.23795604705810547\n",
            "for epoch 39   train loss is  0.25162890553474426      val loss   0.21600230038166046\n",
            "for epoch 40   train loss is  0.2395962029695511      val loss   0.20234239101409912\n",
            "for epoch 41   train loss is  0.2243223637342453      val loss   0.17047081887722015\n",
            "for epoch 42   train loss is  0.22901535034179688      val loss   0.13149216771125793\n",
            "for epoch 43   train loss is  0.19721588492393494      val loss   0.35864633321762085\n",
            "for epoch 44   train loss is  0.21036306023597717      val loss   0.2002478539943695\n",
            "for epoch 45   train loss is  0.2253107726573944      val loss   0.13372591137886047\n",
            "for epoch 46   train loss is  0.21215493977069855      val loss   0.14929243922233582\n",
            "for epoch 47   train loss is  0.21155837178230286      val loss   0.15393802523612976\n",
            "for epoch 48   train loss is  0.2045912742614746      val loss   0.14763693511486053\n",
            "for epoch 49   train loss is  0.1988413780927658      val loss   0.10834063589572906\n",
            "for epoch 50   train loss is  0.17548175156116486      val loss   0.2054261863231659\n",
            "for epoch 51   train loss is  0.1957007348537445      val loss   0.15536826848983765\n",
            "for epoch 52   train loss is  0.18948298692703247      val loss   0.18607673048973083\n",
            "for epoch 53   train loss is  0.2255442887544632      val loss   0.18043258786201477\n",
            "for epoch 54   train loss is  0.17354097962379456      val loss   0.12807008624076843\n",
            "for epoch 55   train loss is  0.17039459943771362      val loss   0.1854892373085022\n",
            "for epoch 56   train loss is  0.1953488439321518      val loss   0.16095565259456635\n",
            "for epoch 57   train loss is  0.21836116909980774      val loss   0.30686110258102417\n",
            "for epoch 58   train loss is  0.18723198771476746      val loss   0.16593024134635925\n",
            "for epoch 59   train loss is  0.15233033895492554      val loss   0.1032528504729271\n",
            "for epoch 60   train loss is  0.1772240251302719      val loss   0.13404425978660583\n",
            "for epoch 61   train loss is  0.15121984481811523      val loss   0.16096417605876923\n",
            "for epoch 62   train loss is  0.14884862303733826      val loss   0.12381833791732788\n",
            "for epoch 63   train loss is  0.15069065988063812      val loss   0.1114683747291565\n",
            "for epoch 64   train loss is  0.1399364024400711      val loss   0.15395842492580414\n",
            "for epoch 65   train loss is  0.14555764198303223      val loss   0.13448455929756165\n",
            "for epoch 66   train loss is  0.1359928548336029      val loss   0.12814614176750183\n",
            "for epoch 67   train loss is  0.16743837296962738      val loss   0.12441398948431015\n",
            "for epoch 68   train loss is  0.13604691624641418      val loss   0.17340897023677826\n",
            "for epoch 69   train loss is  0.16842196881771088      val loss   0.16356119513511658\n",
            "for epoch 70   train loss is  0.1475074589252472      val loss   0.12789693474769592\n",
            "for epoch 71   train loss is  0.14145857095718384      val loss   0.20655469596385956\n",
            "for epoch 72   train loss is  0.1217113584280014      val loss   0.25540536642074585\n",
            "for epoch 73   train loss is  0.11910830438137054      val loss   0.15535125136375427\n",
            "for epoch 74   train loss is  0.14068517088890076      val loss   0.12674376368522644\n",
            "for epoch 75   train loss is  0.17861536145210266      val loss   0.07915838062763214\n",
            "for epoch 76   train loss is  0.1359744817018509      val loss   0.1387307345867157\n",
            "for epoch 77   train loss is  0.14320366084575653      val loss   0.11444763839244843\n",
            "for epoch 78   train loss is  0.13275495171546936      val loss   0.10633101314306259\n",
            "for epoch 79   train loss is  0.1242547407746315      val loss   0.1473790556192398\n",
            "for epoch 80   train loss is  0.11012594401836395      val loss   0.10197367519140244\n",
            "for epoch 81   train loss is  0.12709662318229675      val loss   0.12452248483896255\n",
            "for epoch 82   train loss is  0.137151837348938      val loss   0.09785304963588715\n",
            "for epoch 83   train loss is  0.12395800650119781      val loss   0.17234531044960022\n",
            "for epoch 84   train loss is  0.12280698120594025      val loss   0.12061387300491333\n",
            "for epoch 85   train loss is  0.1312038153409958      val loss   0.1564738005399704\n",
            "for epoch 86   train loss is  0.12157198041677475      val loss   0.07324868440628052\n",
            "for epoch 87   train loss is  0.12364636361598969      val loss   0.10029009729623795\n",
            "for epoch 88   train loss is  0.0981336161494255      val loss   0.08583945035934448\n",
            "for epoch 89   train loss is  0.11077062040567398      val loss   0.0955524891614914\n",
            "for epoch 90   train loss is  0.12783721089363098      val loss   0.10720906406641006\n",
            "for epoch 91   train loss is  0.1022433191537857      val loss   0.057360123842954636\n",
            "for epoch 92   train loss is  0.11299099773168564      val loss   0.1117473617196083\n",
            "for epoch 93   train loss is  0.15130263566970825      val loss   0.13286785781383514\n",
            "for epoch 94   train loss is  0.11126265674829483      val loss   0.1610642969608307\n",
            "for epoch 95   train loss is  0.1244766116142273      val loss   0.12093393504619598\n",
            "for epoch 96   train loss is  0.13220110535621643      val loss   0.06782960146665573\n",
            "for epoch 97   train loss is  0.09506086260080338      val loss   0.09566958248615265\n",
            "for epoch 98   train loss is  0.11310825496912003      val loss   0.2141062468290329\n",
            "for epoch 99   train loss is  0.1571449637413025      val loss   0.10548819601535797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  test_logits = modelGIN(myGraph)\n",
        "  test_loss = loss_fn_GIN(test_logits[myGraph.ndata['test_mask']], myGraph.ndata['labels'][myGraph.ndata['test_mask']])\n",
        "  print(f\"Test Loss = {test_loss.item()}\")\n",
        "\n",
        "  _, predicted = torch.max(test_logits[myGraph.ndata['test_mask']], dim=1)\n",
        "  accuracy = (predicted == myGraph.ndata['labels'][myGraph.ndata['test_mask']]).float().mean()\n",
        "  print(f\"Test Accuracy = {accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "c-fpwUPzkYtq",
        "outputId": "c03315c3-5671-4040-a3ce-3035444e4ca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss = 0.2001727819442749\n",
            "Test Accuracy = 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdyN1BSauXnF",
        "outputId": "65f2ca41-c516-4523-ef3e-490afd7b62ca"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.6.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n"
      ],
      "metadata": {
        "id": "vqNSUcqeuIw8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, num_heads):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(in_dim, hidden_dim, heads=num_heads)\n",
        "        self.conv2 = GATConv(hidden_dim * num_heads, out_dim, heads=1)\n",
        "        self.dropout=nn.Dropout(0.2)\n",
        "    def forward(self, data,d_rate=0.2):\n",
        "        x, edge_index = data.ndata['features'], data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=d_rate, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "yVSiLe-lsQ_P"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 4"
      ],
      "metadata": {
        "id": "OvIEliV_vpjr"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3DyMnIrypREs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelGAT = GAT(in_feats,hid_feats,out_feats,num_heads)\n",
        "loss_fn_GAT = nn.CrossEntropyLoss()\n",
        "optimizer_GAT = torch.optim.Adam(modelGAT.parameters(),lr=.003)\n",
        "schedulerGAT = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n"
      ],
      "metadata": {
        "id": "pP13Gwk3udvU"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=100"
      ],
      "metadata": {
        "id": "SjP9_d7Fl_Fg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_lossGAT = float('inf')\n",
        "patienceGAT = 10\n",
        "min_deltaGAT = 0.01\n",
        "counterGAT = 0"
      ],
      "metadata": {
        "id": "QBx0rBsNqURQ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(num_epochs):\n",
        "  modelGAT.train()\n",
        "  logits = modelGAT(myGraph)\n",
        "  loss = loss_fn_GAT(logits[myGraph.ndata['train_mask']], myGraph.ndata['labels'][myGraph.ndata['train_mask']])\n",
        "  optimizer_GAT.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer_GAT.step()\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    val_logits = modelGAT(myGraph)\n",
        "    val_loss = loss_fn_GAT(val_logits[myGraph.ndata['val_mask']], myGraph.ndata['labels'][myGraph.ndata['val_mask']])\n",
        "  schedulerGAT.step()\n",
        "  if val_loss < best_val_lossGAT - min_deltaGAT:\n",
        "        best_val_lossGAT = val_loss\n",
        "        counterGAT = 0\n",
        "  else:\n",
        "        counterGAT += 1\n",
        "\n",
        "  if counterGAT >= patienceGAT:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "  print(\"for epoch\",e,\"  train loss is \",loss.item(),\"     val loss  \",val_loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhmGdRFywf59",
        "outputId": "b648243d-3fa9-45e7-ab4a-293b6d9d1871"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for epoch 0   train loss is  0.07195504754781723      val loss   0.016204100102186203\n",
            "for epoch 1   train loss is  0.06451189517974854      val loss   0.028879040852189064\n",
            "for epoch 2   train loss is  0.11055926978588104      val loss   0.04016738384962082\n",
            "for epoch 3   train loss is  0.06824623048305511      val loss   0.04810553789138794\n",
            "for epoch 4   train loss is  0.06089162081480026      val loss   0.06885048002004623\n",
            "for epoch 5   train loss is  0.07339777052402496      val loss   0.07488447427749634\n",
            "for epoch 6   train loss is  0.0945415124297142      val loss   0.06005709245800972\n",
            "for epoch 7   train loss is  0.05407755821943283      val loss   0.04824599623680115\n",
            "for epoch 8   train loss is  0.06264400482177734      val loss   0.04115303233265877\n",
            "for epoch 9   train loss is  0.05885685607790947      val loss   0.027575263753533363\n",
            "Early stopping triggered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  test_logits = modelGAT(myGraph)\n",
        "  test_loss = loss_fn_GAT(test_logits[myGraph.ndata['test_mask']], myGraph.ndata['labels'][myGraph.ndata['test_mask']])\n",
        "  print(f\"Test Loss = {test_loss.item()}\")\n",
        "\n",
        "  _, predicted = torch.max(test_logits[myGraph.ndata['test_mask']], dim=1)\n",
        "  accuracy = (predicted == myGraph.ndata['labels'][myGraph.ndata['test_mask']]).float().mean()\n",
        "  print(f\"Test Accuracy = {accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "HDnseuGB7WcL",
        "outputId": "ec0baf60-1160-4100-f009-50b86cb96db8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss = 0.058197982609272\n",
            "Test Accuracy = 100.00%\n"
          ]
        }
      ]
    }
  ]
}